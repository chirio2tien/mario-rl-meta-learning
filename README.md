# Mario RL Meta-Learning

Deep Reinforcement Learning cho Super Mario Bros sá»­ dá»¥ng Meta-Learning (Reptile/MAML), PPO vÃ  DQN vá»›i PyTorch.

## ğŸ¯ TÃ­nh nÄƒng

- âœ… Há»— trá»£ nhiá»u thuáº­t toÃ¡n RL: **PPO**, **DQN/Rainbow**, **Reptile Meta-Learning**
- âœ… Kiáº¿n trÃºc modular, dá»… má»Ÿ rá»™ng
- âœ… Xá»­ lÃ½ áº£nh Ä‘áº§y Ä‘á»§: grayscale, resize, frame stacking
- âœ… Quáº£n lÃ½ train/test levels riÃªng biá»‡t
- âœ… Logging vá»›i TensorBoard
- âœ… Vectorized environments cho training song song
- âœ… Unit tests

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
mario-rl-meta-learning/
â”œâ”€â”€ configs/           # Cáº¥u hÃ¬nh YAML vÃ  danh sÃ¡ch levels
â”œâ”€â”€ envs/             # Environment wrappers
â”œâ”€â”€ agents/           # CÃ¡c thuáº­t toÃ¡n RL
â”œâ”€â”€ models/           # Neural network architectures
â”œâ”€â”€ buffers/          # Replay vÃ  rollout buffers
â”œâ”€â”€ meta/             # Meta-learning components
â”œâ”€â”€ utils/            # Utilities (logger, scheduler...)
â”œâ”€â”€ tests/            # Unit tests
â”œâ”€â”€ train.py          # Script huáº¥n luyá»‡n chÃ­nh
â””â”€â”€ evaluate.py       # Script Ä‘Ã¡nh giÃ¡
```

## ğŸš€ CÃ i Ä‘áº·t

### YÃªu cáº§u
- Python 3.8+
- CUDA 11+ (khuyáº¿n nghá»‹ cho GPU)

### CÃ¡c bÆ°á»›c cÃ i Ä‘áº·t

```bash
# Clone repository
git clone https://github.com/chirio2tien/mario-rl-meta-learning.git
cd mario-rl-meta-learning

# Táº¡o virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c: venv\Scripts\activate  # Windows

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
```

## ğŸ“– Sá»­ dá»¥ng

### Huáº¥n luyá»‡n PPO
```bash
python train.py --config configs/ppo.yaml
```

### Huáº¥n luyá»‡n DQN
```bash
python train.py --config configs/dqn.yaml
```

### Huáº¥n luyá»‡n Reptile Meta-Learning
```bash
python train.py --config configs/reptile.yaml
```

### ÄÃ¡nh giÃ¡ model
```bash
python evaluate.py --checkpoint checkpoints/best_model.pth --levels test
```

## âš™ï¸ Cáº¥u hÃ¬nh

Chá»‰nh sá»­a file YAML trong `configs/`:

**PPO** (`configs/ppo.yaml`):
- Learning rate, clip range, entropy coefficient
- Number of parallel environments
- Rollout length, minibatch size

**DQN** (`configs/dqn.yaml`):
- Epsilon-greedy schedule
- Replay buffer size, batch size
- Target network update frequency

**Reptile** (`configs/reptile.yaml`):
- Inner/outer learning rates
- Number of inner steps
- Task batch size

**Levels** (`configs/levels.py`):
- Train levels: dÃ¹ng cho meta-training
- Test levels: Ä‘Ã¡nh giÃ¡ generalization
- Validation levels: early stopping

## ğŸ“Š Káº¿t quáº£

_ThÃªm káº¿t quáº£ thá»±c nghiá»‡m, Ä‘á»“ thá»‹ learning curves, video demo táº¡i Ä‘Ã¢y._

## ğŸ—ï¸ Kiáº¿n trÃºc chi tiáº¿t

### Environment Pipeline
```
Raw Frame â†’ Grayscale â†’ Resize(84x84) â†’ FrameStack(4) â†’ SkipFrame(4) â†’ Policy Network
```

### Meta-Learning Flow (Reptile)
```
Outer Loop (over task distribution):
  Sample batch of levels
  For each level:
    Inner Loop: Adapt policy (5 gradient steps)
    Store adapted weights
  Meta-update: Aggregate weights
```

## ğŸ§ª Testing

```bash
# Cháº¡y táº¥t cáº£ tests
pytest tests/

# Test cá»¥ thá»ƒ
pytest tests/test_env.py -v
```

## ğŸ“š TÃ i liá»‡u tham kháº£o

- [Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347)
- [Rainbow DQN](https://arxiv.org/abs/1710.02298)
- [Reptile Meta-Learning](https://arxiv.org/abs/1803.02999)
- [gym-super-mario-bros](https://github.com/Kautenja/gym-super-mario-bros)

## ğŸ“ License

MIT License

## ğŸ‘¤ TÃ¡c giáº£

**chirio2tien**
- GitHub: [@chirio2tien](https://github.com/chirio2tien)

## ğŸ¤ ÄÃ³ng gÃ³p

Contributions, issues vÃ  feature requests Ä‘á»u Ä‘Æ°á»£c hoan nghÃªnh!

---

â­ Náº¿u project há»¯u Ã­ch, hÃ£y cho má»™t star nhÃ©!